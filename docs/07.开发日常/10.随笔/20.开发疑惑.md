---
title: 开发疑惑
date: 2023-07-16 09:56:32
permalink: /pages/170fbd/
author: 
  name: 泪伤荡
  link: https://github.com/gemini-ming
---
# 开发疑惑

## 什么是 POJO

POJO（Plain Old Java Object）是一种普通的 Java 对象，通常是**指与数据模型相关的类**。

其中可能涉及以下几个部分：

1. DTO（Data Transfer Object）：DTO 是用于在不同层之间传输数据的对象。它通常包含了与数据库表字段一一对应的属性，并提供了用于数据传输的 getter 和 setter 方法。
2. Entity：Entity（实体类）通常用于映射数据库中的表或文档。它包含了与数据库表字段一一对应的属性，并提供了持久化操作的方法。Entity 对象一般用于 ORM（对象关系映射）框架，如 Hibernate，来进行数据库操作。
3. VO（Value Object）：VO 是一种用于封装某个特定领域的值或数据的对象。它可以包含多个属性，并且这些属性可以是不同的数据类型。VO 对象通常用于在领域模型之间传递数据，而不直接与数据库交互。

这些对象（DTO、Entity、VO）都可以被归类为 POJO，因为它们是普通的 Java 对象，不依赖于特定的框架或技术。它们仅包含数据字段和相应的访问方法，不包含业务逻辑或特定的操作。

## 持久化与半持久化

- **持久化**将数据写入非易失性存储介质中，以确保数据在系统**重启后仍然可用**，
- 而**半持久化**只在系统运行期间保留数据，并在系统关闭或**重启时丢失**。

持久化适用于需要长期保存数据的场景，而半持久化适用于临时数据或不需要长期存储的场景。

## RocketMQ 集群部署问题

在 RocketMQ 中，推荐部署三台服务器的原因是为了实现高可用性和数据冗余。这种部署方式通常被称为主从复制或者主从模式。

部署三台服务器的好处是，当一台服务器发生故障或者需要维护时，系统可以继续正常运行。其中两台服务器承担主从节点的角色，而第三台服务器则可以作为备用节点。这样，即使有一台服务器宕机，消息仍然可以在其他服务器上进行处理和传递，**确保消息的可靠性和持久性**。

另外，将主从节点部署在两台服务器上也有其优势。通过将主节点和从节点部署在不同的服务器上，可以**实现数据的冗余和负载均衡**。主节点负责消息的写入和处理，而从节点则负责消息的备份和复制。这样可以提高系统的吞吐量和性能，并降低单点故障的风险。

总结起来，RocketMQ 推荐部署三台服务器并在两台服务器上部署主从节点，旨在提供**高可用性**、数据冗余和负载均衡，以确保消息系统的可靠性和性能。

## 什么是异步编排

异步编排是一种编程模式或设计方法，用于处理异步操作和任务的并发执行和协调。

- 在传统的同步编程中，代码会按照顺序逐行执行，当遇到耗时的操作时，程序会阻塞等待操作完成后再继续执行下一步。
- 而异步编排则允许在执行一个异步操作时，不会阻塞程序的其他部分，而是可以继续执行其他任务。

异步编排的目的是提高程序的并发性和性能，特别是在处理多个独立异步任务或操作时。它可以帮助程序充分利用系统资源，以提高处理速度和响应性。

在异步编排中，通常会使用以下方法或技术：

1. **回调函数：** 在执行异步任务时，可以指定一个回调函数，该函数会在任务完成后被调用，用于处理任务的结果。
2. **Promise/Future：** 使用 Promise 或 Future 对象可以在执行异步任务后获取其结果，而不需要阻塞程序的其他部分。
3. **异步事件处理：** 使用事件驱动模式来处理异步操作的完成事件，通过注册监听器或订阅事件来处理任务的结果。
4. **异步任务队列：** 将异步任务放入队列中，由专门的线程或进程来处理，以充分利用系统资源和提高并发性。
5. **并行计算：** 在执行多个独立异步任务时，可以将它们并行处理，以加快任务的完成时间。

异步编排在现代编程中得到广泛应用，特别是在处理网络请求、数据库查询、IO 操作等涉及到耗时的任务时，通过异步编排可以提高程序的性能和响应性，使得程序能够更高效地处理并发操作。

## 分布式的含义是什么

分布式的含义是指将一个系统或应用拆分成多个独立的组件（或子系统），这些组件可以在不同的计算机或服务器上运行，并通过网络进行通信和协作。每个组件负责特定的功能或任务，通过相互之间的通信和协作，实现整个系统的功能。

在分布式系统中，各个组件可以独立运行，这使得系统更加灵活、可扩展和容错。分布式系统通常涉及多台计算机，每台计算机可以处理一部分工作负载，从而提高系统的性能和吞吐量。

总的来说，分布式系统是一种将**复杂的系统拆分为多个独立组件**，分别运行在不同计算机上，通过网络相互通信和协作，以提高系统的性能、可扩展性和可靠性的**设计架构**。

## 雪花算法和 UUID

雪花算法（Snowflake Algorithm）和 UUID（Universally Unique Identifier）都是用于生成唯一标识符（ID）的方法，但它们在生成方式、长度和适用场景上存在一些区别。

雪花算法是一种**基于时间戳和机器标识的算法**，用于生成趋势递增、有序且唯一的 ID。它使用一个 64 位的整数表示 ID，其中包含时间戳、数据中心 ID、机器 ID 和序列号。通过将这些部分合理地组合在一起，雪花算法可以在分布式系统中生成全局唯一的 ID。由于雪花算法基于时间戳，因此生成的 ID 保留了时间上的排序性。这种算法适合于分布式系统，以确保生成的 ID 在分布式环境中的唯一性。

UUID 是一种标准化的**全局唯一标识符**，它的长度为 128 位（16 字节），通常表示为 32 个十六进制数字的字符串。UUID 是由计算机硬件地址、当前时间戳和随机数等信息生成的。UUID 的生成过程是基于随机性的，因此可以在分布式环境中保持全局唯一性。与雪花算法不同，UUID 并不保留时间上的排序性，因此生成的 ID 是无序的。UUID 适用于需要在不同系统、不同数据库之间保持唯一性的场景。

选择使用雪花算法还是 UUID 取决于具体的需求和应用场景：

- 如果需要在分布式系统中保持全局唯一性并具有趋势递增的特性，可以选择雪花算法。
- 如果需要在不同系统、不同数据库之间保持全局唯一性，并且无需关注排序性和时间相关的特性，可以选择 UUID。

选择自增整数或 UUID 作为主键 ID 取决于以下因素：

- 需求：根据具体需求来判断。如果需要在数据库层面保证递增的唯一性，可以选择自增整数。如果需要在分布式环境下保证全局唯一性，可以选择 UUID。
- 性能：自增整数通常比 UUID 在索引、查询和排序等方面更高效，因为它们可以利用数据库的自增特性和整数索引。UUID 则相对较大，可能会对数据库性能产生一定的影响。
- 可读性：自增整数主键具有可读性，可以更容易地理解和推断出记录的创建顺序。UUID 则相对较长，不具备直观的可读性。

## 局部变量没有默认值，那包装类型的局部变量呢

局部变量无论是基本数据类型还是包装类型，在 Java 中都没有默认值。局部变量在声明时必须显式地初始化，否则编译器会报错。

对于基本数据类型的局部变量，如果在声明时没有初始化，编译器会提示错误。例如：

```java
public void exampleMethod() {
    int x; // 编译错误，x未初始化
    System.out.println(x);
}
```

对于包装类型的局部变量，同样没有默认值，并且也必须显式地初始化。例如：

```java
public void exampleMethod() {
    Integer y; // 编译错误，y未初始化
    System.out.println(y);
}
```

需要注意的是，类的成员变量（包括静态成员变量和实例成员变量）会有默认值，但局部变量不会。成员变量的默认值是根据数据类型来确定的，例如，整数类型的成员变量默认为 0，布尔类型的成员变量默认为 false，引用类型的成员变量默认为 null 等。但这些规则不适用于局部变量。

## 何为高可用，何为高性能？

1. 高可用（High Availability）：

高可用性指系统或服务在长时间运行过程中，保持持续可用的能力，即系统在面对各种故障或异常情况时，能够继续提供服务而不发生中断或停机。**高可用性是为了确保系统在任何时间都能够可靠地运行，以满足用户的需求**。

为实现高可用性，通常需要采取一系列措施，包括但不限于：

- 使用冗余设计：例如备份服务器、集群部署等，当一个节点出现故障时，其他节点能够顶替其工作，确保服务的持续性。
- 异地多活部署：将系统在不同地理位置进行部署，以防止单点故障和灾难性故障的影响。
- 自动故障切换：当出现故障时，自动切换到备用系统或节点，尽量减少人工干预和停机时间。
- 监控与报警：建立健全的监控系统，及时发现问题并采取措施解决。

2. 高性能（High Performance）：

**高性能指系统或服务能够在给定的资源限制下，以更快的速度完成任务或处理更多的请求**。高性能系统通常具有更低的响应时间和更高的吞吐量，能够更好地满足用户对于效率和速度的要求。

为实现高性能，可以考虑以下方面：

- 优化算法和数据结构：使用更高效的算法和数据结构，以减少计算和存储资源的消耗。
- 并发处理：充分利用多核处理器和多线程技术，实现并行处理，提高系统的并发能力。
- 缓存：使用缓存技术减少对底层存储系统的访问，加快数据的获取速度。
- 负载均衡：合理分配请求到不同的处理节点，避免单个节点过载，保持系统的稳定和高效运行。

高可用性和高性能都是构建稳定、高效的系统和服务的重要目标。

- 高可用性确保系统持续可用，不会因为故障而中断，
- 而高性能则追求在资源限制下以更快的速度完成任务和处理请求。

## Nginx 能做什么？

Nginx（发音为 "engine x"）是一个高性能的开源反向代理服务器和 Web 服务器软件，具有轻量级、高并发、低内存消耗等特点。

Nginx 主要用于以下几个方面：

1. **反向代理**：Nginx 作为反向代理服务器，可以接收客户端请求并转发到后端的多个应用服务器，从而实现负载均衡和高可用性。

2. **Web 服务器**：Nginx 本身也可以作为 Web 服务器，用于直接处理和响应客户端的 HTTP 请求，返回静态或动态内容。

3. **负载均衡**：Nginx 支持多种负载均衡算法，如轮询、IP Hash 等，可以将请求分发到后端的多个服务器，均衡负载，提高系统的性能和可扩展性。

4. **缓存加速**：Nginx 可以缓存静态内容，如图片、CSS、JavaScript 等，从而减轻后端服务器的负担，加快内容的访问速度。

5. SSL 终结：Nginx 可以作为 SSL(一种加密协议) 终结代理，接收加密的 HTTPS 请求，解密后将请求转发到后端 HTTP 服务器。

6. 反向代理缓存：Nginx 可以缓存后端服务器的响应内容，从而减轻后端服务器的压力，提高响应速度。

7. 动态模块支持：Nginx 支持动态模块，可以通过安装插件扩展其功能，例如支持 HTTP/2、Gzip 压缩、GeoIP 等。

8. **URL 重写和重定向**：Nginx 支持强大的 URL 重写和重定向功能，可以通过配置规则对 URL 进行修改和重定向。

## 为什么 Controller 数据传输通常是用 JSON 格式？

主要原因包括：

1. **数据格式统一性：** JSON 提供了一种统一的数据格式，无论是前端还是后端，都可以使用相同的格式来传输数据，这简化了数据交换和处理的复杂性。
2. **跨语言支持：** JSON 是一种跨语言的数据格式，几乎所有编程语言都支持 JSON 的解析和生成，这使得不同技术栈的应用能够轻松地进行数据交流。
3. **轻量级：** JSON 是一种轻量级的数据格式，相对于 XML 等其他数据格式来说，它的数据结构更简洁，占用的空间更小，传输效率更高。
4. **易于阅读和调试：** JSON 的文本表示形式相对容易阅读和理解，这在调试和排查问题时非常有用。
5. **前后端分离：** 在现代 Web 开发中，通常采用前后端分离的架构。前端通常使用 JavaScript 来处理数据，而 JSON 恰好是 JavaScript 对象的文本表示，因此非常适合前后端分离的场景。
6. **RESTful API：** JSON 与 RESTful API 结合紧密。RESTful API 通常使用 HTTP 请求和 JSON 响应来进行通信，这使得数据的传输和处理更加直观和简单。
7. **丰富的数据结构：** JSON 支持多种数据类型，包括字符串、数字、布尔值、数组和对象，这使得它能够表示复杂的数据结构和关系型数据。
8. **生态系统支持：** JSON 有丰富的工具和库支持，可以方便地在各种开发环境中使用。

总的来说，JSON 具有轻量级、跨语言、易于阅读和处理等特点。

*其他格式*

1. **XML：** XML 是另一种常见的数据传输格式，它具有丰富的数据验证和类型信息，适用于某些需要强类型数据的场景。在某些行业标准中，如SOAP协议中，仍然广泛使用XML。
2. **Protocol Buffers（Protobuf）：** Protobuf 是一种二进制数据传输格式，它具有高效的编解码性能和紧凑的数据表示。它通常用于需要高性能和小尺寸数据传输的场景。
3. **MessagePack：** MessagePack 也是一种二进制数据传输格式，类似于 JSON，但更紧凑和高效。它适用于需要快速数据传输的场景。
4. **Thrift：** Thrift 是一种多语言支持的跨语言数据传输框架，它支持多种数据格式，并提供了数据传输的代码生成工具。它常用于大规模分布式系统中。
5. **GraphQL：** GraphQL 是一种用于 API 查询的查询语言和运行时环境，它允许客户端指定返回的数据结构，避免了过度或不足的数据传输。
6. **自定义数据格式：** 根据特定需求，您还可以设计和使用自定义的数据传输格式，以满足项目的特殊要求。

*以下是 JSON 字符串在网络传输中的一般流程：*

1. **数据封装**：JSON 字符串可以通过各种网络协议进行传输，包括 HTTP、WebSocket、MQTT 等。在 HTTP 中，JSON 数据通常作为 HTTP 请求的正文（Request Body）或响应的一部分来传输。数据在传输过程中可以经过加密和压缩等处理以提高安全性和性能。
2. **数据传输**：JSON 字符串可以通过各种网络协议进行传输，包括 HTTP、WebSocket、MQTT 等。在 HTTP 中，JSON 数据通常作为 HTTP 请求的正文（Request Body）或响应的一部分来传输。数据在传输过程中可以经过加密和压缩等处理以提高安全性和性能。
3. **数据接收**：在接收端，网络应用程序接收 JSON 字符串。这可以是服务器接收来自客户端的请求，或者客户端接收来自服务器的响应。接收的 JSON 字符串需要解析以将其转换回应用程序可用的数据结构。编程语言通常提供了解析 JSON 字符串的库或函数。例如，在 JavaScript 中，可以使用 `JSON.parse()` 来将 JSON 字符串解析为 JavaScript 对象。
4. **数据处理**：接收的 JSON 数据可以用于数据呈现、更新 UI、计算、存储等各种用途，具体取决于应用程序的需求。

## DevOps 是什么？

"DevOps" 是 "Development"（开发）和 "Operations"（运维）两个词的组合，是一种软件开发和运维领域的实践和文化，旨在改进开发团队和运维团队之间的协作、通信和自动化流程，以缩短软件系统的开发周期、提高质量、降低成本。

*DevOps* 

DevOps 通过自动化工具和流程的使用，使开发和运维更加紧密合作，以便更快地交付软件更新和新功能，同时保持稳定的系统运行。关键原则和实践包括：

1. **持续集成（CI）**：开发人员频繁地将代码合并到主干，并通过自动化构建和测试流程确保代码的质量。
2. **持续交付（CD）**：自动化软件交付流程，以便在任何时候都能够快速、可靠地部署新代码。
3. **自动化**：自动化各种重复的任务，包括部署、配置管理、测试和监视。
4. **监视和反馈**：实时监视应用程序性能和系统健康，以便及早发现和解决问题。
5. **协作**：促进开发和运维之间的紧密协作，减少沟通障碍。

DevOps 的目标是**加速软件开发和发布过程**，使组织能够更快地适应市场需求，提供更好的用户体验，并降低运维成本。它已经成为现代软件开发中的一个关键实践，并在各种行业中得到广泛采用。

*Jenkins* 

Jenkins 是 DevOps 领域中非常重要的开源工具和产品之一。Jenkins 旨在自动化软件构建、测试和部署过程，以支持**持续集成（CI）和持续交付（CD）**。以下是关于 Jenkins 的一些关键信息：

1. **持续集成（CI）**：Jenkins 允许开发团队频繁地将代码合并到主干，并在每次提交后*自动*构建、测试和部署应用程序。这有助于尽早发现和解决潜在问题，并确保代码的质量。
2. **可扩展性**：Jenkins 提供了大量的插件和扩展，使它可以与各种不同的工具和技术集成，包括版本控制系统、构建工具、测试框架、容器化平台等。
3. **自动化**：Jenkins 允许用户配置自动化构建和部署任务，这些任务可以根据特定的触发条件（如代码提交或定时触发）自动执行。
4. **分布式构建**：Jenkins 支持在多个代理节点上分布式执行构建任务，以加速构建过程。
5. **社区支持**：Jenkins 是一个开源项目，有庞大的社区支持，可以获得广泛的文档和插件支持。
6. **可视化**：Jenkins 提供了用户友好的 Web 界面，使用户能够轻松查看构建和部署任务的状态以及日志输出。
7. **持续交付（CD）**：Jenkins 可以集成到持续交付流程中，以实现自动化部署到各种环境，包括测试、预生产和生产环境。

Jenkins 被广泛用于各种软件开发和运维场景中，它帮助团队提高交付速度、减少人工干预、降低错误率，并支持*敏捷开发*和 DevOps 实践。

- 它是 DevOps 工具链中的一个关键组成部分，
- 与版本控制系统（如Git）、
- 容器化平台（如Docker）、
- 自动化配置管理工具（如Ansible）等工具一起使用，实现了持续集成和持续交付的目标。

## 前端请求和集群

**关系**

- 前端和集群之间的关系是通过网络通信实现的。前端通过使用HTTP协议或其他通信协议，将请求发送到后端服务所在的集群中。
- 负载均衡器负责将请求分发给集群中的某一个后端服务实例，后端服务实例接收请求、处理业务逻辑，并将响应发送回前端。

**集群问题**

在一个集群中，可以有多个服务实例运行在不同的机器上，每个服务实例都可以拥有不同的 IP 地址和端口号。一种常见的做法是使用负载均衡器，通过负载均衡器将请求分发到不同的服务实例上。

以下是集群中服务的一些可能情况：

1. **相同 IP，不同端口：**
   - 不同服务实例运行在相同的机器上，但使用不同的端口号。例如，一个服务实例可能监听在 `192.168.1.100:8080`，另一个服务实例监听在 `192.168.1.100:8081`。
2. **相同端口，不同 IP：**
   - 不同服务实例运行在不同的机器上，但使用相同的端口号。例如，一个服务实例可能监听在 `192.168.1.100:8080`，另一个服务实例监听在 `192.168.1.101:8080`。
3. **不同 IP，不同端口：**
   - 不同服务实例运行在不同的机器上，且使用不同的端口号。例如，一个服务实例可能监听在 `192.168.1.100:8080`，另一个服务实例监听在 `192.168.1.101:8081`。
4. **负载均衡：**
   - 使用负载均衡器来分发请求，负载均衡器通常拥有一个公共的 IP 地址和端口号，而后端的服务实例可以有不同的 IP 和端口号。负载均衡器会根据某种算法（如轮询、最小连接数等）将请求分发到不同的服务实例上。

选择不同的部署方式取决于具体的需求和架构设计。使用负载均衡器可以实现请求的分发和故障转移，同时允许服务实例在不同的机器上运行。这样的设计提高了系统的可伸缩性和可用性。